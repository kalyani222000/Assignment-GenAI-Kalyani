{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b407980c",
   "metadata": {},
   "source": [
    "##To do\n",
    "1. Read native pdf\n",
    "2. Get the open ai _key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "7fc3f45b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openai 0.27.6\n"
     ]
    }
   ],
   "source": [
    "!openai --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5c309b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "# openai.api_key = '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'\n",
    "os.environ[\"OPENAI_API_KEY\"] = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce154190",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tika import parser  \n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "# opening pdf file\n",
    "parsed_pdf = parser.from_file(\"Data/Employee Handbook .pdf\")\n",
    "\n",
    "# parsed_pdf['content'] returns string \n",
    "data = parsed_pdf['content'] \n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter()\n",
    "paragraphs = splitter.split_text(text=data)\n",
    "\n",
    "def clean_text(text):\n",
    "    cleaned_string = text.replace(\"\\n\",\"\").replace('..',\"\")\n",
    "    return cleaned_string\n",
    "cleaned_paragraphs = [clean_text(para) for para in paragraphs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa9b120d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if the document is big\n",
    "import os\n",
    "import openai\n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be3fb91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    return openai.Embedding.create(input = [text], model=model)['data'][0]['embedding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b5ec07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 200\n",
    "\n",
    "def sentence_splitter(all_text):\n",
    "    shortened_sentences = []\n",
    "    words = all_text.split()\n",
    "    if len(words) > max_words:\n",
    "        # Split sentence into smaller chunks\n",
    "        num_chunks = len(words) // max_words + 1\n",
    "        chunk_size = len(words) // num_chunks\n",
    "        for i in range(num_chunks):\n",
    "            start_idx = i * chunk_size\n",
    "            end_idx = (i + 1) * chunk_size\n",
    "            chunk = ' '.join(words[start_idx:end_idx])\n",
    "            shortened_sentences.append(chunk)\n",
    "    else:\n",
    "        shortened_sentences.append(sentence)\n",
    "    return(shortened_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40184886",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def generate_embeddings(shortened_sentences):\n",
    "    book_embeddings = []\n",
    "    for text in tqdm(shortened_sentences):\n",
    "        book_embeddings.append(get_embedding(text))\n",
    "    return(book_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d32cb5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "def get_relevant_sentences(query,embeddings,shortened_sentences):\n",
    "    embeddings = np.vstack(embeddings)\n",
    "    embedding_dim = 1536\n",
    "    index = faiss.IndexFlatL2(embedding_dim) # L2 distance index\n",
    "    index.add(embeddings)\n",
    "    query_embedding =  torch.Tensor(get_embedding(query))# numpy array of shape (embedding_dim,)\n",
    "    k = 2 # Number of nearest neighbors to retrieve\n",
    "    D, I = index.search(query_embedding.reshape(1, -1), k) # Search for nearest neighbors\n",
    "    print(D,I)\n",
    "    # Extract answers from retrieved books\n",
    "    retrieved_indices = I[0] # Indices of retrieved books in the index\n",
    "    retrieved_sentences = [shortened_sentences[index] for index in retrieved_indices] # Extract retrieved books from book data\n",
    "    relevant_text = \" \".join(retrieved_sentences)\n",
    "    index = []\n",
    "    return(relevant_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31e28473",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_from_text(query,text):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[ \n",
    "            {\n",
    "                \"role\": \"system\", \"content\": f\"You are a helpful assistant that answer the question based on the context provided, answer only if the answer is given in the ocntext don't use outside information and tell sorry if the question is outside the domain\",\n",
    "                \"role\": \"user\", \"content\": f\"Given this context: {text} answer this {query}\"},\n",
    "        ],\n",
    "    )\n",
    "    answer = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    return(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79eacae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from langchain import OpenAI, PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "import openai\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# openai.api_key = \"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\"\n",
    "llm = OpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c57ea79",
   "metadata": {},
   "source": [
    "Summary Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96df8121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_pdf(pdf_file_path, custom_prompt=\"\"):\n",
    "    loader = PyPDFLoader(pdf_file_path)\n",
    "    documents = loader.load()\n",
    "    chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
    "    summary = chain.run(documents) \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4045ecad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' HashedIn Technologies is an employee and customer-centric organization that provides its campus recruits with policies and procedures for EPF, laptop policy, loan policy, confidentiality policy, exit policy and process, policy against sexual and workplace harassment, policy on women work timings, grievance/suggestion box, nepotism check, visitor management process, business travel process, and Employee Care Center (ECC) letter. It also provides guidelines for taking leave, maternity leave, paternity leave, special sick leave, sabbatical leave, compensatory off, absenteeism, absconding, bonus payments, referral bonus, work from home policy, career track change, certification reimbursement, medical and personal accident insurance, and the Medibuddy Doctor On Call Program.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize_pdf('Data/Employee Handbook .pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0eaf8f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pypdf2 in c:\\users\\kaldwivedi\\python_projects\\genai\\venv\\lib\\site-packages (3.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pypdf2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b86980f",
   "metadata": {},
   "source": [
    "## Question Answering with Session Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1b6fe080",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO answer any question related to document we can directly use langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a9cd5d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from langchain.llms import OpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53a4089a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "# load document\n",
    "\n",
    "loader = PyPDFLoader(\"Data/Employee Handbook .pdf\")\n",
    "\n",
    "documents = loader.load()\n",
    "\n",
    "# split the documents into chunks\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents (documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8db7caf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "db = FAISS.from_documents(texts, embeddings)\n",
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":2})\n",
    "ga = ConversationalRetrievalChain.from_llm(OpenAI(), retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "51acb99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "# query = \"what is the leave policy of hashedin?\"\n",
    "# result = ga({\"question\": query, \"chat_history\": chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "048410f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter your querywhat is the leave policy of hashedin?\n",
      "{'question': 'what is the leave policy of hashedin?', 'chat_history': [], 'answer': ' Hashers are offered an allotment of 12 working days in a year under a common pool for CL and SL that can be used flexibly by them as CL and/or SL. Leave days are accrued for each complete calendar month and credited at the beginning of the month. Any weekend or public holiday that might occur during sabbatical and maternity leave shall be considered as part of the leave. Weekends or public holidays that might occur during earned leave shall NOT be considered as part of the leave.'}\n",
      "Please enter your query\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    query=input(\"Please enter your query\")\n",
    "    \n",
    "    if not query:\n",
    "        break\n",
    "        \n",
    "    result = ga({\"question\": query, \"chat_history\": chat_history})\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "297f7290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'what is the leave policy of hashedin?',\n",
       " 'chat_history': [],\n",
       " 'answer': \" Hashedin's leave policy offers all eligible professionals a total of 30 days of leave per calendar year. Hashers can avail up to 18 days of Privileged Leaves (PLs) and 12 days of Casual Leaves (CLs)/Sick Leaves (SLs) combined together as per their needs. CL/SL accrual is 1 day per month regardless of the date of joining and Public Holidays are covered by the leave policy depending on the type of coverage.\"}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "96abf6ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'what question did I asked u earlier?',\n",
       " 'chat_history': [('what is the leave policy of hashedin?',\n",
       "   \" Hashedin's leave policy offers all eligible professionals a total of 30 days of leave per calendar year. Hashers can avail up to 18 days of Privileged Leaves (PLs) and 12 days of Casual Leaves (CLs)/Sick Leaves (SLs) combined together as per their needs. CL/SL accrual is 1 day per month regardless of the date of joining and Public Holidays are covered by the leave policy depending on the type of coverage.\")],\n",
       " 'answer': ' Hashedin offers all eligible professionals a total of 30 days of leave per calendar year. Hashers can avail upto 18 days of Privileged leaves (PLs) and 12 days of Casual Leaves (CLs) and Sick Leaves (SLs) combined together as per needs. CL/SL accrual is 1 day per month, and blanket coverage is applied for weekends and public holidays that occur during sabbatical and maternity leave. Public holidays are not covered for earned leave.'}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95203467",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
